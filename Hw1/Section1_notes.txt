Purpose of this section
A word prediction program

What is an n-gram?
is a contiguous sequence of n items from given sample of text or speech.
For example a sequence of two words like "please turn" is called a 2-gram, while a sequence of three words like "please turn your" is a 3-gram
N-grams are widely used in natural language processing (NLP) tasks to analyze text data
```
Take this sentence:
"the whale is white"
Break it into words:
the | whale | is | white
```
It take that sentence and uses it to predicate the next word
For example:
1-gram (Unigram) looks at the word:
the -> ?
whale -> ?
predicating the next word is hard of you only have 1-gram

2-gram (Bigram) looks at two words at a time
(the → whale)
(whale → is)
(is → white)
When I see the, what word usually comes next?

3-gram (Trigram) looks at three words at a time
(the whale → is)
(whale is → white)
When I see the whale, what word usually comes next?

First Step
First Step of the program we need to read the text file.
n-gram will not work if we dont have the text file.
First open the file then read (r) the file
```
with open("moby_dick.txt", "r", encoding="utf-8") as file:
    text = file.read()
```
- open(...) → opens the file
- "r" → read mode
- encoding="utf-8" → avoids weird character bugs
- file.read() → reads everything into a string
- text → now holds the entire book

Second Step
We will need to tokenize the text bc currently it is a string and n-gram will not work in string
What tozenization mean is to split the string to individual words/text
```
Call me Ishmael. Some years ago...
to
["call", "me", "ishmael", ".", "some", "years", "ago", "..."]
```
Another reason we do tozenization is because we will need to separate punctuation form the words.
If we dont, the model will think ex. code? != code, even when the words are the same, n-gram won't recongize it bc of the ?
To do this we will need to use regular expression (re)
```
r"\b\w+\b|[^\w\s]"
```
- \b\w+\b → matches words (letters/numbers between word boundaries)
- [^\w\s] → matches anything that is not a word or space (punctuation)
- | → “or”, so we get **either words or punctuation”


Third Step
Counting Bigrams. A bigram is two word sequence. counting bigrams means to find two sequence words that are frequent
We want probabilities
```
Example: If “call” is always followed by “me”, then:
P("me" | "call") = 1.0

If “are” is followed by “cute”, “smart”, “playful” once each:
P("cute" | "are") = 1/3
```
We need to do a dictionary, a dictionary is from the collections module
```
bigram_counts = defaultdict(lambda: defaultdict(int))
```
unlike regular dictionary, what defaultdict do is when u try to access a key that doesnt exit, it automatically creates it with deafult values.
lambda is a anonymous function that you can use once:
```
Normally, you define a function with def and give it a name:

def add_one(x):
    return x + 1

print(add_one(4)) 
```
An anonymous function is a function without a name.
You don’t assign it a regular name with def.
Instead, you write it “inline” wherever you need it.
```
Example:
f = lambda x: x + 1
print(f(4))  # Output: 5

or

print((lambda x: x + 1)(4))  # Output: 5
```
Why use anonymous functions?
Short, quick functions you only need once.
instead of creating a name function that only run once and never use it again, we can insetad call lambda so it basically acts as a replacement of a name function, and u can use it once
basically to save code space, and more efficient
So having another deafultdict inside a deafultdict help count the number of bigrams
```
Example:
sentence: "I like AI"
Bigrams: ("I", "like"), ("like", "AI")
{
    'I': {'like': 1},
    'like': {'AI': 1}
}
```

To find the counting bigram, this nexted defaultdict is neccessary
```
Sentence: "I like AI and I like Python"
Bigram: ("I", "like"), ("like", "AI"), ("AI", "and"), ("and", "I"), ("I", "like"), ("like", "Python")

{
    'I': {'like': 2},            # 'I' is followed by 'like' twice
    'like': {'AI': 1, 'Python': 1},  # 'like' is followed by 'AI' once and 'Python' once
    'AI': {'and': 1},
    'and': {'I': 1}
}
```


Step Four
Determine the probabilities of the n-gram counting words


Step Five
Predict the next word using both greedy and categorical sampling

Greedy:
    What key=next_words.get means:
    key is an optional function that tells max() how to compare elements.
    next_words.get is a function that returns the value for a given key in the dictionary.
    ```
    Step by step:
    max(next_words, key=next_words.get)
    Iterates over keys of next_words → 'like', 'love'

    Compares them based on their value in the dictionary:
    'like' → next_words.get('like') = 2
    'love' → next_words.get('love') = 1
    Returns the key with the maximum value → 'like'
    ```

Categorical sampling
    words = list(next_words.keys())
    weights = list(next_words.values())
    return random.choices(words, weights=weights, k=1)[0]
    
    ```
    Example:
    words = list(next_words.keys())   # ['like', 'love']
    weights = list(next_words.values())  # [2, 1]
    ```

    random.choices(words, weights=weights, k=1)
    ```
    Picks k elements randomly from words
    weights tells Python how likely each word is:
    k=1 → pick one word
    Returns a list with 1 element: ['like'] or ['love']

    >>> random.choices(['like', 'love'], weights=[2,1], k=1)
    ['like']
    >>> random.choices(['like', 'love'], weights=[2,1], k=1)[0]
    'like
    ```

```
"""
CSC 4700 Homework 1: N-Gram Models and BPE
Author: Daniel Guo
Instructor: Dr. Keith Mills

Section 1: N-Gram Models
Implement bigram and trigram probabilistic language models and understand how they 
operate on a code-level.

Allowed libraries: Python standard library, numpy, pandas

References: Google, ChatGPT
Google and ChatGPT were used to clarify syntax and usage of standard library functions.
As well as to understand concepts of n-grams
"""


import argparse
import pickle
import random
import re
from collections import defaultdict

# Since lambda functions cannot be pickled, we define a named function to return defaultdict(int)
# pickle cannot save lambda functions as well as functions defined in another function, and pickle can only save things that are globally defined
def int_defaultdict():
    return defaultdict(int)

class NGramModel:
    def __init__(self, n):
        if n not in (2, 3):
            raise ValueError("n must be 2 (bigram) or 3 (trigram)")
        self.n = n # n-gram size
        self.vocab = set() # Vocabulary set, help to track words seen during training so bascially no duplicates words
        self.ngram_counts = defaultdict(int_defaultdict) # Nested dictionary for n-gram counts, lambda doesnt work here because of pickle serialization issues
        self.ngram_probs = {} # Nested dictionary for n-gram probabilities

    # Tokenizes the input text into tokens (words and punctuation).
    def tokenize(self, text): 
        return re.findall(r"\b\w+\b|[^\w\s]", text.lower()) # Lowercase for normalization, \b\w+\b|[^\w\s] - \b\w+\b matches words, [^\w\s] matches punctuation

    # Trains the n-gram model on the provided text.
    def train(self, text):
        # Counts n-gram occurrences in the list of tokens.
        tokens = self.tokenize(text)
        self.vocab = set(tokens)  # Update vocabulary with unique tokens
        for i in range(len(tokens) - self.n + 1): # Loop through tokens to get n-grams
            current_word = tuple(tokens[i:i+self.n-1]) 
            next_word = tokens[i + self.n - 1] 
            self.ngram_counts[current_word][next_word] += 1

        # Converting counts to probabilities.
        """
        Converts a nested dictionary of counts into probabilities.
        Input: {current_word: {next_word: count}}
        Output: {current_word: {next_word: probability}}

        Dictionary comprehension:
        probabilities[current_word] = {
            word: count / total
            for word, count in next_words.items()
        }

        Equivalent normal loop:
        inner_dict = {}
        for word, count in next_words.items():
            inner_dict[word] = count / total
        probabilities[current_word] = inner_dict
        """
        self.ngram_probs = {}
        for current_word, next_words in self.ngram_counts.items():
            total = sum(next_words.values())
            self.ngram_probs[current_word] = {
                word: count / total
                for word, count in next_words.items()
            }

    # Predicts the next word based on the current word and probability dictionary.
    def predict_next_word(self, input, deterministic=False):

        if input not in self.ngram_probs:
            print("Error: Word not found in training data.")
            return ""
        
        next_words = self.ngram_probs[input]
        
        if deterministic:
            # Greedy argmax
            return max(next_words, key=next_words.get) # Get the word with the highest probability
        
        # Categorical sampling
        words = list(next_words.keys())
        weights = list(next_words.values())
        return random.choices(words, weights=weights, k=1)[0]


# Command-line interface for training and predicting with n-gram models.
def main():
    parser = argparse.ArgumentParser(description="N-gram Language Model CLI") # argparse is Python standard library for parsing command-line arguments
    
    # add_argument method defines what arguments the program requires
    parser.add_argument(
        "activity",
        choices=["train_ngram", "predict_ngram"],
        help="Select which activity to perform"
    )
    parser.add_argument("--data", type=str, help="Path to training data") # Path to training data
    parser.add_argument("--save", type=str, help="Path to save trained model") # Path to save trained model
    parser.add_argument("--load", type=str, help="Path to load trained model") # Path to load trained model
    parser.add_argument("--word", type=str, help="Starting word(s) for prediction") # Starting word(s) for prediction ex. --word whale (bigram) or --word whale of (trigram)
    parser.add_argument("--nwords", type=int, help="Number of words to predict") # Number of words to generate during prediction ex. --nwords 10 (generate 10 words)
    parser.add_argument("--n", type=int, choices=[2, 3], help="Order of n-gram") # choices limits input to 2 or 3
    parser.add_argument("--d", action="store_true", help="Use deterministic (greedy) sampling") # action="store_true" means if user includes --d in command line, args.d will be True, otherwise False

    args = parser.parse_args()

    if args.activity == "train_ngram":
        if not args.data or not args.save or not args.n:
            parser.error("train_ngram requires --data, --save, and --n")
        
        # Read file in order to train model
        with open(args.data, "r", encoding="utf-8") as f:
            text = f.read()
        
        # Train model
        model = NGramModel(n=args.n) # Create n-gram model instance base on n value choosen in the parser.add_argument("--n", ...)
        model.train(text)
        
        # Save model after training
        with open(args.save, "wb") as f: # "wb" means write binary mode
            pickle.dump(model, f) # pickle.dump to serialize the model object to file, meaning save the model to disk
        
        print(f"Model trained and saved to {args.save}")

    elif args.activity == "predict_ngram":
        if not args.load or not args.word or not args.nwords:
            parser.error("predict_ngram requires --load, --word, and --nwords")
        
        # Load saved trained model
        with open(args.load, "rb") as f: # "rb" means read binary mode
            model = pickle.load(f) # pickle.load to deserialize the model object from file, meaning load the model back into memory
        
        # Split starting words
        context = tuple(args.word.lower().split()) # Lowercase for normalization, split into tuple of words
        generated = list(context)
        
        for _ in range(args.nwords): # Generate nwords number of words
            # Predict next word using the model, chosen deterministic in the parser.add_argument("--d", ...)
            next_word = model.predict_next_word(tuple(generated[-(model.n-1):]), deterministic=args.d) # gernerated [-(model.n-1):] gets the last n-1 words from generated list
            if not next_word:
                break
            generated.append(next_word)
        
        print(" ".join(generated)) # Join the generated words into a single string and print



if __name__ == "__main__":
    main()
```